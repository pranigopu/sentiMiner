{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews.doRecommend</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>I thought it would be as big as small paper bu...</td>\n",
       "      <td>Too small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>This kindle is light and easy to use especiall...</td>\n",
       "      <td>Great light reader. Easy to use at the beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>Didnt know how much i'd use a kindle so went f...</td>\n",
       "      <td>Great for the price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>I am 100 happy with my purchase. I caught it o...</td>\n",
       "      <td>A Great Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>Solid entry level Kindle. Great for kids. Gift...</td>\n",
       "      <td>Solid entry-level Kindle. Great for kids</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  reviews.doRecommend  reviews.rating  \\\n",
       "0  AVqVGZNvQMlgsOJE6eUY                False               3   \n",
       "1  AVqVGZNvQMlgsOJE6eUY                 True               5   \n",
       "2  AVqVGZNvQMlgsOJE6eUY                 True               4   \n",
       "3  AVqVGZNvQMlgsOJE6eUY                 True               5   \n",
       "4  AVqVGZNvQMlgsOJE6eUY                 True               5   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I thought it would be as big as small paper bu...   \n",
       "1  This kindle is light and easy to use especiall...   \n",
       "2  Didnt know how much i'd use a kindle so went f...   \n",
       "3  I am 100 happy with my purchase. I caught it o...   \n",
       "4  Solid entry level Kindle. Great for kids. Gift...   \n",
       "\n",
       "                                  reviews.title  \n",
       "0                                     Too small  \n",
       "1  Great light reader. Easy to use at the beach  \n",
       "2                           Great for the price  \n",
       "3                                   A Great Buy  \n",
       "4      Solid entry-level Kindle. Great for kids  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../../data/test.csv\")\n",
    "reviewsData = data[['id',\n",
    "                  'reviews.doRecommend',\n",
    "                  'reviews.rating',\n",
    "                  'reviews.text',\n",
    "                  'reviews.title']]\n",
    "reviewsData.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the words within the reviews\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "reviews = reviewsData['reviews.text'].values\n",
    "tokenizer = Tokenizer(num_words = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Tokenizer' class in the 'tensorflow.keras.preprocessing.text' module enables you to tokenize text. Tokenizing text is the process of breaking a text into tokens (usually individual words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__class__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_keras_api_names\n",
      "_keras_api_names_v1\n",
      "char_level\n",
      "document_count\n",
      "filters\n",
      "fit_on_sequences\n",
      "fit_on_texts\n",
      "get_config\n",
      "index_docs\n",
      "index_word\n",
      "lower\n",
      "num_words\n",
      "oov_token\n",
      "sequences_to_matrix\n",
      "sequences_to_texts\n",
      "sequences_to_texts_generator\n",
      "split\n",
      "texts_to_matrix\n",
      "texts_to_sequences\n",
      "texts_to_sequences_generator\n",
      "to_json\n",
      "word_counts\n",
      "word_docs\n",
      "word_index\n"
     ]
    }
   ],
   "source": [
    "# Using dir(tokenizer) to see what attributes this object has\n",
    "for a in dir(tokenizer): print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing texts\n",
    "tokenizer.fit_on_texts(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informative attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokenizer.word_index:\n",
      "('i', 1)\n",
      "('it', 2)\n",
      "('a', 3)\n",
      "('the', 4)\n",
      "('my', 5)\n",
      "('to', 6)\n",
      "\n",
      "tokenizer.word_counts:\n",
      "('i', 12)\n",
      "('thought', 1)\n",
      "('it', 10)\n",
      "('would', 2)\n",
      "('be', 3)\n",
      "('as', 3)\n",
      "\n",
      "tokenizer.word_docs:\n",
      "('read', 2)\n",
      "('recommend', 1)\n",
      "('very', 1)\n",
      "('would', 1)\n",
      "('big', 1)\n",
      "('on', 2)\n"
     ]
    }
   ],
   "source": [
    "# Extra function to print the first few elements of a collection\n",
    "def printn(collection, n):\n",
    "    for i, t in enumerate(collection):\n",
    "        if i > n: break\n",
    "        else: print(t)\n",
    "\n",
    "print(\"\\ntokenizer.word_index:\")\n",
    "printn(tokenizer.word_index.items(), 5)\n",
    "print(\"\\ntokenizer.word_counts:\")\n",
    "printn(tokenizer.word_counts.items(), 5)\n",
    "print(\"\\ntokenizer.word_docs:\")\n",
    "printn(tokenizer.word_docs.items(), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we see that tokenization using the 'Tokenizer' class associates each word with an index, word count and the number of documents i.e. separate texts in which each word occurs. This data is stored in the attributes word_index, word_count and word_docs respectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

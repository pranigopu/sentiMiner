{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ignore warning messages when filtering data\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns of the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN NAMES\n",
      "------------\n",
      "id\n",
      "dateAdded\n",
      "dateUpdated\n",
      "name\n",
      "brand\n",
      "categories\n",
      "primaryCategories\n",
      "manufacturer\n",
      "manufacturerNumber\n",
      "reviews.date\n",
      "reviews.doRecommend\n",
      "reviews.numHelpful\n",
      "reviews.rating\n",
      "reviews.text\n",
      "reviews.title\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# The whole data set\n",
    "\n",
    "# Generalized code for accessing the data directory\n",
    "# (Meant to work even if this file is within some other subdirectory)\n",
    "path = \"data/amazonConsumerReviews.csv\"\n",
    "while True:\n",
    "    try:\n",
    "        data = pd.read_csv(path)\n",
    "        break\n",
    "    except:\n",
    "        path = \"../\" + path\n",
    "print(\"COLUMN NAMES\\n------------\")\n",
    "for c in data.columns: print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only keeping relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews.doRecommend</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>I thought it would be as big as small paper bu...</td>\n",
       "      <td>Too small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>This kindle is light and easy to use especiall...</td>\n",
       "      <td>Great light reader. Easy to use at the beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>Didnt know how much i'd use a kindle so went f...</td>\n",
       "      <td>Great for the price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  reviews.doRecommend  reviews.rating  \\\n",
       "0  AVqVGZNvQMlgsOJE6eUY                False               3   \n",
       "1  AVqVGZNvQMlgsOJE6eUY                 True               5   \n",
       "2  AVqVGZNvQMlgsOJE6eUY                 True               4   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I thought it would be as big as small paper bu...   \n",
       "1  This kindle is light and easy to use especiall...   \n",
       "2  Didnt know how much i'd use a kindle so went f...   \n",
       "\n",
       "                                  reviews.title  \n",
       "0                                     Too small  \n",
       "1  Great light reader. Easy to use at the beach  \n",
       "2                           Great for the price  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only selecting relevant columns\n",
    "reviewsData = data[['id',\n",
    "                  'reviews.doRecommend',\n",
    "                  'reviews.rating',\n",
    "                  'reviews.text',\n",
    "                  'reviews.title']]\n",
    "reviewsData.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow offers a tokenizer class that abstracts the exact implementation of tokenization for a collection of texts. An instance of this class is a callable object that offers many methods and features related to tokenization of texts. In particular, it maintains attributes containing the following:\n",
    "- Total word count\n",
    "- Assigning a unique index number for each word\n",
    "- The number of times a word appears in all the texts\n",
    "- The number of different texts in which a word appears\n",
    "\n",
    "<br><br>\n",
    "**WHY USE TENSORFLOW TOKENIZER?**<br>\n",
    "There are many libraries available for natural language processing, which also provide text mining capacities like tokenization. These libraries include:\n",
    "- Natural Language Toolkit (NLTK)\n",
    "- Gensim\n",
    "- polyglot\n",
    "- TextBlob\n",
    "- CoreNLP\n",
    "- spaCy\n",
    "- Pattern\n",
    "- Vocabulary\n",
    "\n",
    "The main advantage of TensorFlow is that its tokenizer class abstracts the tokenization for an entire collection of texts, and not just a single text, as is the case with the NLTK tokenizer function. Furthermore, information about these texts and the occurrence of each word in these texts is also stored. Lastly, the TensorFlow tokenizer class stores various informative attributes and methods that facilitate processes such as:\n",
    "- Looking up the index for a word, and the word for an index\n",
    "- Encoding texts (facilitated by the above features as well as in-built functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__class__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattribute__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_keras_api_names\n",
      "_keras_api_names_v1\n",
      "char_level\n",
      "document_count\n",
      "filters\n",
      "fit_on_sequences\n",
      "fit_on_texts\n",
      "get_config\n",
      "index_docs\n",
      "index_word\n",
      "lower\n",
      "num_words\n",
      "oov_token\n",
      "sequences_to_matrix\n",
      "sequences_to_texts\n",
      "sequences_to_texts_generator\n",
      "split\n",
      "texts_to_matrix\n",
      "texts_to_sequences\n",
      "texts_to_sequences_generator\n",
      "to_json\n",
      "word_counts\n",
      "word_docs\n",
      "word_index\n"
     ]
    }
   ],
   "source": [
    "# Instantiating tokenizer object\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = 5000)\n",
    "\n",
    "# Viewing all the attributes of this object\n",
    "for d in dir(tokenizer): print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some noteworthy attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filters**<br>\n",
    "The string containing all the characters that will be omitted from consideration.\n",
    "<br><br>\n",
    "**split**<br>\n",
    "The string pattern by which the texts are split.\n",
    "<br><br>\n",
    "**index_word, word_index**<br>\n",
    "'index_word' is dictionary associating every index with the respective word, while 'word_index' is a dictionary associating every word to the respective index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some noteworthy methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.fit_on_texts(\\<collection of texts\\>)**<br>\n",
    "Updates the internal vocabulary of the tokenizer object based on a given collection of texts. This method creates a vocabulary index (based on word frequency), i.e.it fills the values of the dictionaries **index_word** and **word_index**.\n",
    "<br><br>\n",
    "**.texts_to_sequences(\\<collection of texts\\>)**<br>\n",
    "Transforms each text in the collection of texts to a sequence of integers, using the indices given to each word. Note that 'sequence' means an iterable collection (in this case, it would be an iterable collection of indices corresponding to the respective word in the text).\n",
    "<br><br>\n",
    "**.get_config( )**<br>\n",
    "Returns a dictionary containing each attribute of the tokenizer object and their respective values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting indices on texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'the')\n",
      "(2, 'and')\n",
      "(3, 'to')\n",
      "(4, 'it')\n",
      "(5, 'i')\n"
     ]
    }
   ],
   "source": [
    "# Creating internal vocabulary and word indices\n",
    "reviews = reviewsData['reviews.text'].values\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "# Viewing some index-word pairs --only for demo--\n",
    "for i, x in enumerate(tokenizer.index_word.items()):\n",
    "    if i > 4: break\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting texts to sequences (i.e. encoding text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED:\n",
      "[5, 330, 4, 53, 46, 25, 267, 25, 181, 804, 23, 215, 107, 3, 46, 54, 42, 9, 3444, 5, 238, 4, 8, 128, 181, 3, 82, 15, 4, 28, 29, 692, 25, 483, 32, 53, 245, 109, 7, 342, 404] \n",
      "\n",
      "ORIGINAL:\n",
      "I thought it would be as big as small paper but turn out to be just like my palm. I think it is too small to read on it... not very comfortable as regular Kindle. Would definitely recommend a paperwhite instead.\n"
     ]
    }
   ],
   "source": [
    "# Replacing words with their respective indices\n",
    "# (Indices can be seen in the 'word_index' or 'index_word'attributes)\n",
    "encodedDocs = tokenizer.texts_to_sequences(reviews)\n",
    "# NOTE: reviews = reviewsData['reviews.text'].values\n",
    "\n",
    "# Comparing element of 'encodedDocs' to corresponding element of 'reviews'\n",
    "print(\"ENCODED:\")\n",
    "print(encodedDocs[0], \"\\n\")\n",
    "print(\"ORIGINAL:\")\n",
    "print(reviews[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

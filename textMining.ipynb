{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Intro to spaCy:<br>https://spacy.io/usage/spacy-101\n",
    "2. Tokenization & lemmatization using spaCy:<br>https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
    "3. 'Language' class (spaCy):<br>https://spacy.io/api/language\n",
    "4. Comparing NLTK and spaCy:<br>\n",
    "    - https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2\n",
    "    - https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns of the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN NAMES\n",
      "------------\n",
      "id\n",
      "dateAdded\n",
      "dateUpdated\n",
      "name\n",
      "brand\n",
      "categories\n",
      "primaryCategories\n",
      "manufacturer\n",
      "manufacturerNumber\n",
      "reviews.date\n",
      "reviews.doRecommend\n",
      "reviews.numHelpful\n",
      "reviews.rating\n",
      "reviews.text\n",
      "reviews.title\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# The whole data set\n",
    "\n",
    "# Generalized code for accessing the data directory\n",
    "# (Meant to work even if this file is within some other subdirectory)\n",
    "path = \"data/amazonConsumerReviews.csv\"\n",
    "while True:\n",
    "    try:\n",
    "        data = pd.read_csv(path)\n",
    "        break\n",
    "    except:\n",
    "        path = \"../\" + path\n",
    "print(\"COLUMN NAMES\\n------------\")\n",
    "for c in data.columns: print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only keeping relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews.doRecommend</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>I thought it would be as big as small paper bu...</td>\n",
       "      <td>Too small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>This kindle is light and easy to use especiall...</td>\n",
       "      <td>Great light reader. Easy to use at the beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>Didnt know how much i'd use a kindle so went f...</td>\n",
       "      <td>Great for the price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  reviews.doRecommend  reviews.rating  \\\n",
       "0  AVqVGZNvQMlgsOJE6eUY                False               3   \n",
       "1  AVqVGZNvQMlgsOJE6eUY                 True               5   \n",
       "2  AVqVGZNvQMlgsOJE6eUY                 True               4   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I thought it would be as big as small paper bu...   \n",
       "1  This kindle is light and easy to use especiall...   \n",
       "2  Didnt know how much i'd use a kindle so went f...   \n",
       "\n",
       "                                  reviews.title  \n",
       "0                                     Too small  \n",
       "1  Great light reader. Easy to use at the beach  \n",
       "2                           Great for the price  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only selecting relevant columns\n",
    "reviewsData = data[['id',\n",
    "                  'reviews.doRecommend',\n",
    "                  'reviews.rating',\n",
    "                  'reviews.text',\n",
    "                  'reviews.title']]\n",
    "reviewsData.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has a tokenizer class whose instances are a callable objects that accept a string as an argument and return an object of type **Doc** (more precisely, **spacy.tokens.doc.Doc**). Such an object contains various attributes that are applicable to the tokenized words, including conversion to JSON format and text vectorization (i.e. converting tokenized text into its numerical representation).\n",
    "<br><br>\n",
    "Furthermore, spaCy also provides language models that offer powerful features, such as parts-of-speech tagging (i.e. identifying the grammatical role of a word or word sequence in a sentence), noun chunk extraction (i.e. detecting nouns) and named entity recognition (i.e. detecting proper nouns), all of which would lead to easier and more comprehensive language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy vs. NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our perusal of NLTK and spaCy codes, we have come to the conclusion that spaCy could be a more attractive choise due to the following reasons:\n",
    "- **Doc** objects are easier to inspect, transform and extract information from\n",
    "- spaCy's language models help produce more sophisticated and appropriate tokenization\n",
    "- Lemmatization of **Doc** object elements is very straightforward using language models\n",
    "- spaCy aims to provide fewer tools that deliver better performance and developer experience\n",
    "\n",
    "**NOTE ON THE LAST POINT**<br>\n",
    "spaCy is not a comprehensive package for natural language processing (NLP) methods. In other words, it is not suited for researching or learning about different NLP methods and concepts. Rather, it is focused on enhancing implementation, providing what the developers of spaCy think is the best method to achieve a certain NLP task.\n",
    "<br><br>\n",
    "**NOTE ON LANGUAGE MODEL**<br>\n",
    "A statistical language model is a probability distribution of words or word sequences. In practice, a language model gives the probability of a certain word sequence being 'valid' in a given context. Note that validity here is not grammatical validity, but validity with respect to the actual usage of language. In other words, it aims to model how people use language.\n",
    "<br><br>\n",
    "**MAIN FACTORS FOR CHOOSING SPACY**<br>\n",
    "While the above points make spaCy a more attractive choice for using in the development of our application, the main reasons for ultimately choosing spaCy to perform tokenization are two-fold\n",
    "<br><br>\n",
    "Firstly, the availability of language models in spaCy that would enhance the quality and effectiveness of lemmatization (discussed later). Secondly, tokenization using language models applies lemmatization, entity extraction and part-of-speech analysis during tokenization itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy installation using terminal commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pip install spacy** _(Installs spaCy library)_<br>\n",
    "- **python -m spacy download en_core_web_sm** _(Downloads the language model for English)_\n",
    "\n",
    "_(For installation in the Jupyter notebook environment, prefix these commands with !)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(spaCy tokenization using language model)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating instance of tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(This instance uses English language model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy import load\n",
    "# Loading the language model\n",
    "tokenizer = load(\"en_core_web_sm\")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, above tokenizer is not an instance of the **Tokenizer** class present in the **tokenizer** module of spaCy. Instead, it is an instance of the class **English**, which is itself a subclass of the class **Language**. The **English** class implements tokenization using the statistical language model for English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(... along with the simultaneously performed results)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object's type: <class 'spacy.tokens.doc.Doc'>\n",
      "Object's element's type: <class 'spacy.tokens.token.Token'>\n",
      "Size: 13\n",
      "\n",
      "Text:\n",
      "Manchester United is looking to sign Harry Kane for $90 million.\n",
      "\n",
      "Tokens:\n",
      "- Manchester\n",
      "- United\n",
      "- is\n",
      "- looking\n",
      "- to\n",
      "- sign\n",
      "- Harry\n",
      "- Kane\n",
      "- for\n",
      "- $\n",
      "- 90\n",
      "- million\n",
      "- .\n",
      "\n",
      "Entities:\n",
      "(Manchester United, Harry Kane, $90 million)\n",
      "\n",
      "POS tags:\n",
      "Manchester : PROPN\n",
      "United : PROPN\n",
      "is : AUX\n",
      "looking : VERB\n",
      "to : PART\n",
      "sign : VERB\n",
      "Harry : PROPN\n",
      "Kane : PROPN\n",
      "for : ADP\n",
      "$ : SYM\n",
      "90 : NUM\n",
      "million : NUM\n",
      ". : PUNCT\n",
      "\n",
      "Lemmas:\n",
      "Manchester : Manchester\n",
      "United : United\n",
      "is : be\n",
      "looking : look\n",
      "to : to\n",
      "sign : sign\n",
      "Harry : Harry\n",
      "Kane : Kane\n",
      "for : for\n",
      "$ : $\n",
      "90 : 90\n",
      "million : million\n",
      ". : .\n"
     ]
    }
   ],
   "source": [
    "text = \"Manchester United is looking to sign Harry Kane for $90 million.\"\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Inspecting the returned object\n",
    "print(\"Object's type:\", type(tokens))\n",
    "print(\"Object's element's type:\", type(tokens[0]))\n",
    "print(\"Size:\", len(tokens))\n",
    "\n",
    "print(\"\\nText:\")\n",
    "print(tokens.text)\n",
    "\n",
    "# Printing the tokens\n",
    "print(\"\\nTokens:\")\n",
    "for word in tokens: print(\"-\", word)\n",
    "\"\"\"\n",
    "Alternate iteration method (using indices):\n",
    "for i in range(0, len(tokens)): print(\"-\", tokens[i])\n",
    "\"\"\"\n",
    "\n",
    "# Printing the entities\n",
    "print(\"\\nEntities:\")\n",
    "print(tokens.ents)\n",
    "\n",
    "# Printing the part-of-speech tags\n",
    "print(\"\\nPOS tags:\")\n",
    "for word in tokens: print(word, \":\", word.pos_)\n",
    "    \n",
    "# Printing the lemmas for each word\n",
    "print(\"\\nLemmas:\")\n",
    "for word in tokens: print(word, \":\", word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we see that the return value of the callable instance of the class **English** is a **Doc** object that is a collection of **Token** objects. Furthermore, we see that the original text is preserved, and multiple analyses have been performed on the tokens, such as:\n",
    "- Entity extration\n",
    "- POS tagging\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization (lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen above, lemmatization occurs as tokenization is performed. Hence, the lemma of each token is already available in the **lemma_** attribute of the **Token** objects (which are elements of the **Doc** object that contains these tokens). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to perform text mining for all reviews..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "#------------\n",
    "\"\"\"\n",
    "Lemmatization (along with entity extraction and POS tagging)\n",
    "are also performed simultaneously.\n",
    "\"\"\"\n",
    "from spacy import load\n",
    "# Loading the language model\n",
    "tokenizer = load(\"en_core_web_sm\")\n",
    "tokenizedDocs = []\n",
    "\n",
    "# Tokenizing each review\n",
    "for r in reviewsData['reviews.text']:\n",
    "    tokenizedDocs.append(tokenizer(r))\n",
    "#========================\n",
    "# Lemmatization\n",
    "#------------\n",
    "lemmatizedDocs = []\n",
    "\n",
    "# Iterating through each tokenized text\n",
    "for doc in tokenizedDocs:\n",
    "    lemmatizedDoc = []\n",
    "    \n",
    "    # Iterating through each token in the tokenized text\n",
    "    for token in doc:\n",
    "        lemmatizedDoc.append(token.lemma_)\n",
    "    \n",
    "    # Adding the lemmatized text to the list\n",
    "    lemmatizedDocs.append(lemmatizedDoc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

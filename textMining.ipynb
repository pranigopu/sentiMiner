{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importance of normalization:<br>https://towardsdatascience.com/text-normalization-7ecc8e084e31\n",
    "2. Intro to stemming:<br>https://www.geeksforgeeks.org/python-stemming-words-with-nltk/\n",
    "3. Under-stemming & over-stemming<br>https://towardsdatascience.com/stemming-of-words-in-natural-language-processing-what-is-it-41a33e8996e2\n",
    "4. Comparing NLTK and spaCy:<br>\n",
    "    - https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2\n",
    "    - https://www.activestate.com/blog/natural-language-processing-nltk-vs-spacy/\n",
    "5. Intro to spaCy:<br>https://spacy.io/usage/spacy-101\n",
    "6. Tokenization & lemmatization using spaCy:<br>https://stackabuse.com/python-for-nlp-tokenization-stemming-and-lemmatization-with-spacy-library/\n",
    "7. Definition of language models:<br>https://towardsdatascience.com/the-beginners-guide-to-language-models-aa47165b57f9\n",
    "8. 'Language' class (spaCy):<br>https://spacy.io/api/language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ignore warning messages when filtering data\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns of the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMN NAMES\n",
      "------------\n",
      "id\n",
      "dateAdded\n",
      "dateUpdated\n",
      "name\n",
      "brand\n",
      "categories\n",
      "primaryCategories\n",
      "manufacturer\n",
      "manufacturerNumber\n",
      "reviews.date\n",
      "reviews.doRecommend\n",
      "reviews.numHelpful\n",
      "reviews.rating\n",
      "reviews.text\n",
      "reviews.title\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# The whole data set\n",
    "data = pd.read_csv(\"data/amazonConsumerReviews.csv\")\n",
    "print(\"COLUMN NAMES\\n------------\")\n",
    "for c in data.columns: print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only keeping relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviews.doRecommend</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>I thought it would be as big as small paper bu...</td>\n",
       "      <td>Too small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>This kindle is light and easy to use especiall...</td>\n",
       "      <td>Great light reader. Easy to use at the beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVqVGZNvQMlgsOJE6eUY</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>Didnt know how much i'd use a kindle so went f...</td>\n",
       "      <td>Great for the price</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  reviews.doRecommend  reviews.rating  \\\n",
       "0  AVqVGZNvQMlgsOJE6eUY                False               3   \n",
       "1  AVqVGZNvQMlgsOJE6eUY                 True               5   \n",
       "2  AVqVGZNvQMlgsOJE6eUY                 True               4   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I thought it would be as big as small paper bu...   \n",
       "1  This kindle is light and easy to use especiall...   \n",
       "2  Didnt know how much i'd use a kindle so went f...   \n",
       "\n",
       "                                  reviews.title  \n",
       "0                                     Too small  \n",
       "1  Great light reader. Easy to use at the beach  \n",
       "2                           Great for the price  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only selecting relevant columns\n",
    "reviewsData = data[['id',\n",
    "                  'reviews.doRecommend',\n",
    "                  'reviews.rating',\n",
    "                  'reviews.text',\n",
    "                  'reviews.title']]\n",
    "reviewsData.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing text is the process of breaking a text into tokens (usually individual words). This can be achieved using the built-in Python function **.split**, which partitions the texts based on whitespaces. For more advanced partitioning, we can even use the **split** function from the module **re** (regular expressions handling module). However, instead of bothering ourselves with the exact implementation, we will use available tokenization implementations. \n",
    "<br><br>\n",
    "**CHOOSING A SUITABLE IMPLEMENTATION**<br>\n",
    "Python contains two libraries specifically designed for language processing, namely **NLTK**, and **spaCy**. NLTK tokenizer functions take a string (single text) as an input, and returns a processed string (list of sentences or list of words, based on the tokenizer function used). It is a simple method of tokenization that we can easily implement using the **.split** function or using the regular expressions.\n",
    "<br><br>\n",
    "spaCy has a tokenizer class that is optimized for word tokenization. An instance of this tokenizer is a callable object that accepts a string as an argument and returns an object of type **Doc** (more precisely, **spacy.tokens.doc.Doc**). This object contains various attributes that are applicable to the tokenized words, including conversion to JSON format and text vectorization (i.e. converting tokenized text into its numerical representation).\n",
    "<br><br>\n",
    "Furthermore, spaCy also provides language models that offer powerful features, such as parts-of-speech tagging (i.e. identifying the grammatical role of a word or word sequence in a sentence), noun chunk extraction (i.e. detecting nouns) and named entity recognition (i.e. detecting proper nouns), all of which would lead to easier and more comprehensive language processing.\n",
    "<br><br>\n",
    "However, based on our perusal of NLTK and spaCy codes, we have come to the conclusion that spaCy is a more attractive choise due to the following reasons:\n",
    "- **Doc** objects are easier to inspect, transform and extract information from\n",
    "- spaCy's language models help produce more sophisticated and appropriate tokenization\n",
    "- Lemmatization of **Doc** object elements is very straightforward using language models\n",
    "- spaCy aims to provide fewer tools that deliver better performance and developer experience\n",
    "\n",
    "**NOTE ON THE LAST POINT**<br>\n",
    "spaCy is not a comprehensive package for natural language processing (NLP) methods. In other words, it is not suited for researching or learning about different NLP methods and concepts. Rather, it is focused on enhancing implementation, providing what the developers of spaCy think is the best method to achieve a certain NLP task.\n",
    "<br><br>\n",
    "**NOTE ON LANGUAGE MODEL**<br>\n",
    "A statistical language model is a probability distribution of words or word sequences. In practice, a language model gives the probability of a certain word sequence being 'valid' in a given context. Note that validity here is not grammatical validity, but validity with respect to the actual usage of language. In other words, it aims to model how people use language.\n",
    "<br><br>\n",
    "**MAIN FACTOR FOR CHOOSING SPACY**<br>\n",
    "While the above points make spaCy a more attractive choice for using in the development of our application, the main reason for ultimately choosing spaCy to perform tokenization is availability of language models in spaCy that would enhance the quality and effectiveness of lemmatization (discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy installation using terminal commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pip install spacy** _(Installs spaCy library)_<br>\n",
    "- **python -m spacy download en_core_web_sm** _(Downloads the language model for English)_\n",
    "\n",
    "_(For installation in the Jupyter notebook environment, prefix these commands with !)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy tokenization using language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Creating instance of tokenizer that uses English language model**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy import load\n",
    "# Loading the language model\n",
    "tokenizer = load(\"en_core_web_sm\")\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, above tokenizer is not an instance of the **Tokenizer** class present in the **tokenizer** module of spaCy. Instead, it is an instance of the class **English**, which is itself a subclass of the class **Language**. The **English** class implements tokenization using the statistical language model for English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Tokenization + simultaneous entity extraction**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object's type: <class 'spacy.tokens.doc.Doc'>\n",
      "Object's element's type: <class 'spacy.tokens.token.Token'>\n",
      "Size: 13\n",
      "\n",
      "Text:\n",
      "Manchester United is looking to sign Harry Kane for $90 million.\n",
      "\n",
      "Tokens:\n",
      "- Manchester\n",
      "- United\n",
      "- is\n",
      "- looking\n",
      "- to\n",
      "- sign\n",
      "- Harry\n",
      "- Kane\n",
      "- for\n",
      "- $\n",
      "- 90\n",
      "- million\n",
      "- .\n",
      "\n",
      "Entities:\n",
      "(Manchester United, Harry Kane, $90 million)\n"
     ]
    }
   ],
   "source": [
    "text = \"Manchester United is looking to sign Harry Kane for $90 million.\"\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "# Inspecting the returned object\n",
    "print(\"Object's type:\", type(tokens))\n",
    "print(\"Object's element's type:\", type(tokens[0]))\n",
    "print(\"Size:\", len(tokens))\n",
    "\n",
    "print(\"\\nText:\")\n",
    "print(tokens.text)\n",
    "\n",
    "# Printing the tokens\n",
    "print(\"\\nTokens:\")\n",
    "for word in tokens: print(\"-\", word)\n",
    "\"\"\"\n",
    "Alternate iteration method (using indices):\n",
    "for i in range(0, len(tokens)): print(\"-\", tokens[i])\n",
    "\"\"\"\n",
    "\n",
    "# Printing the entities\n",
    "print(\"\\nEntities:\")\n",
    "print(tokens.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we see that the return value of the callable instance of the class **English** is a **Doc** object that is a collection of **Token** objects. Furthermore, we see that the original text is preserved and the entities are extracted (accurately in this case) and stored in the list 'ents', which is an attribute of the **Doc** object. However, it is clear that for larger texts, the memory requirement for this tokenization process will be high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is the process of converting a token into its base form, which involves \n",
    "- Removing inflections from a word to obtain the root word\n",
    "- Replacing abbreviations with their actual meaning\n",
    "- Identify informal intensifiers such as all-caps and character repetitions\n",
    "- Special tokens such as hashtags, user tags, and URLs are replaced by placeholders<br>_**NOTE**: These placeholders indicate the token type that has been substituted._\n",
    "\n",
    "The goal of normalization is to reduce redundancies in the data, thereby facilitating the deep learning process that uses this data. Redundant or irrelevant data being processed in the machine learning algorithm can affect the speed and accuracy of the learning process, since they are insignificant contributors to the sentiment of the text. Yet, weights of the neural network may be adjusted by traversing these redundant or irrelevant data as inputs to the network, and due to the insignificant contribution of this data to the sentiment, the changes may cause the weights to adjust without causing improvements in the accuracy of the model, potentially even increasing the error of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of substituting a word for its root word i.e. stem. For example, a stemming algorithm reduces the words 'running', 'ran' and 'run' to the stem, 'run', and the words 'retrieval', 'retrieved', 'retrieves' to the stem 'retrieve'. There are many available stemming algorithms (also called stemmers), each with its own advantages and disadvantages, such as:\n",
    "- Porter stemmer\n",
    "- Lovins stemmer\n",
    "- Dawson stemmer\n",
    "- N-gram stemmer\n",
    "- Snowball stemmer\n",
    "\n",
    "**POTENTIAL ISSUES**:<br>\n",
    "**Over- stemming** is when a word is reduced more than appropriate, which leads to two or more words being reduced to the same root word or stem (incorrectly) when they should have been reduced to two or more stem words. For example, 'university' and 'universe' are supposed to be considered as different roots, since their meanings are significantly distinct.\n",
    "<br><br>\n",
    "**Under-stemming** is when two or more words are wrongly reduced to more than one root word, when they actually stem from the same root word. For example, 'data' and 'datum' stem from 'dat', but some algorithms may reduce these words to 'dat' and 'datu' respectively, which is wrong. Both of these have to be reduced to the same stem 'dat'. However, note that trying to optimize such models might lead to over-stemming.\n",
    "<br><br>\n",
    "**CHOOSING THE RIGHT STEMMING ALGORITHM**<br>\n",
    "Some stemmers are more applicable for a particular language (ex. Porter stemmer was designed for English), while others are designed to be applicable for any language, even when it is unknown (ex. snowball stemmer). Since our application was only intended for English language text sources, and since sentiment analysis techniques we are using are designed only for English language texts, we will use stemmers designed for English.\n",
    "<br><br>\n",
    "Furthermore, the expression of sentiment can be quite complex and subtle, and having an excess of words is preferable to not having key words that could potentially affect the sentiment significantly. Hence, for analysing sentiment, under-stemming is preferable to over-stemming. Keeping this in mind, we shall use a less aggressive stemmer, such as **Porter stemmer**.\n",
    "<br><br>\n",
    "_A Python implementation of Porter stemmer is available in the 'stem' module of the 'nltk' library._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Intro to autodifferentation:<br>\n",
    "- https://www.tensorflow.org/guide/autodiff\n",
    "- https://www.neidinger.net/SIAMRev74362.pdf\n",
    "- https://jmlr.org/papers/volume18/17-468/17-468.pdf\n",
    "2. Symbolic differentiation:<br>https://en.wikipedia.org/wiki/Computer_algebra\n",
    "3. Numerical differentiation:<br>https://en.wikipedia.org/wiki/Numerical_differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # For accessing TensorFlow library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Includes discussions on surrounding and related concepts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning methods require the evaluation of derivatives _(ex. calculating the gradient descent i.e. gradient of the loss function of a model)_. In the context of computers, a function is a program or subroutine that takes some input and produces a corresponding output. Here, differentiation means calculating the gradient of a computation with respect to some inputs. $n$th order differentiation of a function can be done in the following ways:\n",
    "- Symbolic differentiation\n",
    "- Numerical differentiation\n",
    "- Automatic differentiaion (also called algorithmic differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symbolic differentiation** is the differentiation of a symbolic mathematical expression using the mathematical techniques, such as differentiation rules, factorization and simplification. Converting a computer program into a viable mathematical expression is itself a major challenge, making this method undesirable differentiating functions in the context of computers. However, this method offers more mathematical rigour, making it more useful in scientific computations.\n",
    "<br><br>\n",
    "**Numerical differentiation**, i.e. the method of finite differences, is the differentiation of a function using a finite number of difference quotients i.e. $\\frac{f(x + h)-f(x)}{h}$, or other related formulae (here, $x$ refers to the input, $f$ refers to the function, $h$ refers to the step-value). For example, to calculate the gradient of a function at a point (i.e. for a given input value), difference quotients are taken a finite number of times for a certain range of values around the input value, using a fixed step-value i.e. $h$.\n",
    "<br><br>\n",
    "These difference quotients are then used to provide an estimate for the gradient at the desired point (either through averaging the difference quotients, or through some other method). The main problem with this method is the round-off error, due to the discrete step-values used, and due to the fact that the range of values around the required point cannot be arbitrarily small.\n",
    "<br><br>\n",
    "**Automatic differentiation**, also called computational differentiation, algorithmic differentiation, or simply autodifferentiation or autodiff, is a set of techniques to evaluate the derivative of a function specified by a computer program. It exploits the fact that every computer program (of any complexity) executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division...) and elementary functions (exp, log, sin, cos...). By applying the chain rule repeatedly to these operations, derivatives of any order can be computed automatically i.e. as the function itself is being evaluated. The results are accurate to working precision, and use at a few times more arithmetic operations than the original program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**In the context of a neural network**_, to differentiate the loss function automatically, the program needs to remember what operations happen in what order during the forward pass (i.e. the calculation process through which output is produced from given input i.e. neural network layers are traversed from input layer to output layer). Then, during the backward pass (i.e. backpropagation), the program can traverse this list of operations in reverse order to compute gradients for each layer and hence for the whole model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides the **GradientTape** API _(that creates an interface between the program and the TensorFlow library)_ for autodifferentiation, which involves the calculation of the gradient of a computation process with respect to some inputs, usually TensorFlow **variables**.\n",
    "<br><br>Note that this is a tool to perform autodifferentiation, not a tool to track autodifferentiation. Rather, it helps keep track of relevant operations that would constitute a function (computation).\n",
    "<br><br>\n",
    "TensorFlow contains the means to record relevant operations executed inside the context (i.e. scope) of a **GradientTape** onto a  data structure called **tape**. TensorFlow then uses the **tape** to calculate the gradients of a recorded computation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 5. 16. 33.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Independent variable of the computation\n",
    "x = tf.Variable([1, 2, 3], dtype = tf.float32)\n",
    "\n",
    "# Computations put in the context of a GradientTape API call\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2 + x**3\n",
    "    \n",
    "# Evaluating the gradient of y = x**2 + x**3 for each value of x (i.e. 1, 2 and 3)\n",
    "\"\"\"\n",
    "Note that gradient of y with respect to x in this case would be given by\n",
    "y'= 2*x + 3*x**2\n",
    "Using this derivative, we can confirm the results of autodifferentiation below.\n",
    "\"\"\"\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE 1: INDEPENDENT VARIABLE REQUIREMENTS**\n",
    "<br>**1.1.**<br>\n",
    "The order of the arguments in the **.gradient** function determines the independent variable of the function.\n",
    "<br>**1.2.**<br>\n",
    "Note that the **.gradient** function looks for the attribute **\\_id** in the independent variable. All object types or collection types do not have this attribute. Hence, we must use TensorFlow object types such as **Variable** or **Tensor** or **Module**, which do have this attribute.\n",
    "<br>**1.3.**<br>\n",
    "Autodifferentiation is only performed for floating point values. Hence, ensure that the **dtype** attribute of the independent variable is some form of floating point.\n",
    "<br><br>\n",
    "**NOTE 2: PERSISTENCE OF RESULT**<br>\n",
    "Note that the **GradientTape** API call is not persistent i.e. the results retrieved from the endpoint (i.e. link to a service) **gradient** is not stored, and are only retrieved for a single call. To make these results persistent, you can assign them to an identifier, as in **d = tape.gradient(y, x)**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

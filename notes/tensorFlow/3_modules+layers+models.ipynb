{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Intro to modules, layers and models:<br>https://www.tensorflow.org/guide/intro_to_modules\n",
    "2. TensorFlow layers:<br>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer\n",
    "3. tf.random.normal function:<br>https://www.tensorflow.org/api_docs/python/tf/random/normal\n",
    "4. Rectified linear unit:<br>https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/\n",
    "5. Sequential model:<br>https://keras.io/guides/sequential_model/#:~:text=A%20Sequential%20model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # For accessing TensorFlow library\n",
    "import numpy as np # For accessing NumPy library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow, most implementations of layers and models, such as Keras or Sonnet, are built on the same foundational class defined in TensorFlow i.e. **Module**. Modules, and by extension, layers and models, are deep-learning terminology for objects. They have internal state, and methods that use that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer is a function with a known mathematical structure that\n",
    "- can be reused\n",
    "- have trainable variables\n",
    "\n",
    "In other terms, a layer is a callable object that takes one or more tensors as input and outputs one or more tensors. It involves\n",
    "- a computation process (defined in the call( ) method)\n",
    "- a state (a set of weight variables assigned certain values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning in TensorFlow requires the definition, creation, storage and restoration of models. Practically speaking, a model is usually the implementation of some machine learning algorithm. Generally speaking, a model can be defined in one of the two following ways:\n",
    "- A function that computes something on tensors (i.e. a forward pass)\n",
    "- A collection of variables that can be updated in response to training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both layers and models are subclasses of the **Module** class. The following shows the definition of a simple **Module** subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XYZ(tf.Module):\n",
    "    # Subclass constructor\n",
    "    def __init__(self, name):\n",
    "        # Applying the parent class' constructor i.e. Module's constructor\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        # Trainable variables\n",
    "        self.trainable1 = tf.Variable([1, 2], name=\"train me 1\")\n",
    "        self.trainable2 = tf.Variable([1, 2], name=\"train me 2\")\n",
    "        \n",
    "        # Non-trainable variable\n",
    "        self.nontrainable1 = tf.Variable(3, trainable=False, name=\"don't train me 1\")\n",
    "        self.nontrainable2 = tf.Variable(3, trainable=False, name=\"don't train me 2\")\n",
    "    #------------------------\n",
    "    # Defining object behaviour as a callable object\n",
    "    \"\"\"\n",
    "    Note that this is not strictly necessary.\n",
    "    It simply enables us to define the function or mathematical structure\n",
    "    of a layer or model in a manner that is very convenient to call and use,\n",
    "    since the object becomes directly callable.\n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        return self.trainable1*x + self.nontrainable2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on '\\_\\_call\\_\\_'**<br>\n",
    "**\\_\\_call\\_\\_** is a built-in method provided by Python, like **\\_\\init\\_\\_**. The **\\_\\_call\\_\\_** method defined in the subclass defines the behaviour of this subclass as a callable object. In other words, having defined the **\\_\\_call\\_\\_** method in the subclass, you can call any instance of the subclass as a function, and the behaviour of this function is defined by the **\\_\\_call\\_\\_** method. Hence, if **m** is an instance of the subclass, **m( )** will be equivalent to **m.\\_\\_call\\_\\_( )**. Note that the instance has all the functionalities of a normal object as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 9], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the subclass\n",
    "simpleModule = XYZ(name = 'owo')\n",
    "\n",
    "# Calling the object\n",
    "simpleModule(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we see the computation defined in the **\\_\\_call\\_\\_** method being performed with the argument **x** being 3. The return value is a tensor, since the result of operation on variables is a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on naming module subclasses**<br>\n",
    "Note that when providing a name for a **Module** subclass, you must ensure the name is a valid Python identifier i.e. the name should begin with an alphabet and only contain alphanumeric characters or underscore, and must not contain special characters or whitespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers within models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a rudimentary neural network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(tf.Module):\n",
    "    def __init__(self, nrow, ncol, name):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        # Normally distributed weights...\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.normal([nrow, ncol], 2, 4),\n",
    "            # => tf.random.normal(shape=[nrow, ncol], mean=2, stddev=4)\n",
    "            name='weights')\n",
    "        \n",
    "        # Biases (initialized as zero)...\n",
    "        self.b = tf.Variable(tf.zeros([nrow, ncol]), name='biases')\n",
    "    #------------------------\n",
    "    # Defining layer behaviour as a callable object\n",
    "    def __call__(self, x):\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        # matmul => matrix multiplication\n",
    "        return tf.nn.relu(y)\n",
    "        # The above applies the ReLu function to the weighted and biased aggregate y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on tf.random.normal**<br>\n",
    "A TensorFlow function that outputs a tensor containing random values from a normal distribution. By default, the normal distribution has _mean = 0, standard deviation = 1_. You can specify the shape of the output, the mean and standard deviation of the distribution, the data type of the values (**dtype**), the seed (for pseudo-random number generation) and the name of the output tensor.\n",
    "<br><br>\n",
    "**Note on ReLU**<br>\n",
    "ReLU i.e. rectified linear unit is a piecewise linear function that will output the input directly if it is positive, and will output zero otherwise. Note that in a neural network, the activation function is responsible for transforming the summed weighted input from the node into the either\n",
    "- the activation status of the node\n",
    "- output of the node for the given input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name=name)\n",
    "        self.layer1 = MyLayer(nrow=3, ncol=3, name=name)\n",
    "        self.layer2 = MyLayer(nrow=3, ncol=2, name=name)\n",
    "    #------------------------\n",
    "    # Defining model behaviour as a callable object\n",
    "    def __call__(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on sequential model**<br>\n",
    "A sequential model is a model containing a simple stack of layers, where each layer has exactly one input tensor and one output tensor, and where the output of one layer becomes the input of the next layer in the stack in a sequential manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the subclass i.e. creating the model\n",
    "m = MyModel(name=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on input & output of the model**<br>\n",
    "Note that in the above case, we cannot give an argument of any shape when calling the model. This is because the model calls the layer instances, which in turn apply matrix multiplication\n",
    "between the argument and the layer's weights that are arranged according to a specified shape. We must consider how matrix addition with the biases affects the result of the matrix multiplication. We must also consider that the output from one layer must be compatible as the input for the net layer. Furthermore, the data types of the argument and the weights must match. Since the weights are of float data type, the argument passed should also be of float data type.\n",
    "<br><br>\n",
    "Now, note that each layer instantiated within the model has 3 rows, which means any input we provide to the model must have exactly 3 columns, so that matrix multiplication is possible between the argument and the matrix of weights. Now, note that matrix addition with biases will broadcast the result of the matrix multiplication, but only if the result of the matrix multiplication is one of the following:\n",
    "1. Single-valued constant\n",
    "2. Has same number of columns, and perfectly divides the number of rows in the bias matrix\n",
    "3. Has same number of rows, and perfectly divides the number of columns in the bias matrix\n",
    "\n",
    "_(Broadcasting is discussed in the document on tensors)_<br>\n",
    "The biases of the first layer is in a 3x3 matrix. Since the input must have exactly 3 columns, it can have either 1 row or 3 rows, by the 3rd condition. In any case, the output of layer 1 will be a 3x3 matrix, since any 1x3 matrix would be broadcasted into a 3x3 matrix due to the matrix addition with the 3x3 bias matrix (for a demonstration, check the demo code section at the end of the document). Any 3x3 matrix input to layer 2 will be converted to a 3x2 matrix through matrix multiplication with the weights of layer 2, which are in a 3x2 matrix. This can be neatly added to the biases of layer 3, which is also in a 3x2 matrix.\n",
    "<br><br>\n",
    "_Hence, for the model, we can only input collections of shapes 1x3 or 3x3. The final output will be a tensor of shape 3x2._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: tf.Tensor(\n",
      "[[ 62.47554 395.267  ]\n",
      " [ 62.47554 395.267  ]\n",
      " [ 62.47554 395.267  ]], shape=(3, 2), dtype=float32)\n",
      "Result 2: tf.Tensor(\n",
      "[[ 39.805847 225.71663 ]\n",
      " [  6.347664  68.57765 ]\n",
      " [113.83667  461.36655 ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Calling the model...\n",
    "print(\"Result 1:\", m(tf.constant([[2.3, 4.1, 5.4]]))) # Tensor with 1 row and 3 columns\n",
    "print(\"Result 2:\", m(tf.constant([[1, 2, 3],\n",
    "                                  [1, 1, 1],\n",
    "                                  [4, 0, 5]], dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incompatible shapes: [2,3] vs. [3,3] [Op:AddV2]\n"
     ]
    }
   ],
   "source": [
    "# Calling a model with the wrong shape of arguments...\n",
    "try: print(\"Result 3:\", m(tf.constant(np.matrix([[1, 2, 3], [4, 0, 5]]), dtype=tf.float32)))\n",
    "except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically generated collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any subclass of **Module**, any **Variable** or **Module** instances assigned as the properties of the subclass (hence of its instances) are automatically collected for any instance of the subclass. In other words, **Module** instances will automatically and recursively collect any **Variable** or **Module** instances assigned to it. These collections include _trainable\\_variables_, _variables_ and _submodules_..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 'simpleModule' i.e. instance of 'XYZ':\n",
      "------------\n",
      "\n",
      "Trainable variables:\n",
      "<tf.Variable 'train me 1:0' shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)>\n",
      "<tf.Variable 'train me 2:0' shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)>\n",
      "\n",
      "All variables:\n",
      "<tf.Variable 'don't train me 1:0' shape=() dtype=int32, numpy=3>\n",
      "<tf.Variable 'don't train me 2:0' shape=() dtype=int32, numpy=3>\n",
      "<tf.Variable 'train me 1:0' shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)>\n",
      "<tf.Variable 'train me 2:0' shape=(2,) dtype=int32, numpy=array([1, 2], dtype=int32)>\n",
      "========================\n",
      "From 'm' i.e. instance of 'MyModel':\n",
      "------------\n",
      "\n",
      "All submodules:\n",
      "<__main__.MyLayer object at 0x1450849a0>\n",
      "<__main__.MyLayer object at 0x109e56070>\n"
     ]
    }
   ],
   "source": [
    "print(\"From 'simpleModule' i.e. instance of 'XYZ':\\n------------\")\n",
    "print(\"\\nTrainable variables:\")\n",
    "for x in simpleModule.trainable_variables: print(x)\n",
    "print(\"\\nAll variables:\")\n",
    "for x in simpleModule.variables: print(x)\n",
    "# (simpleModule has variables but no submodules)\n",
    "print(\"========================\")\n",
    "print(\"From 'm' i.e. instance of 'MyModel':\\n------------\")\n",
    "print(\"\\nAll submodules:\")\n",
    "for x in m.submodules: print(x)\n",
    "# (m has submodules but no variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This allows you to save and load variables, and also create collections of **Modules**. This opens doors to features like:\n",
    "- Managing collections of **Modules** with a single model instance\n",
    "- Saving whole models and model states\n",
    "- Adding layers to models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting by matrix addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before adding b:\n",
      "tf.Tensor([[17.946648    0.02306306 13.761355  ]], shape=(1, 3), dtype=float32)\n",
      "\n",
      "After adding b: \n",
      "tf.Tensor(\n",
      "[[17.946648    0.02306306 13.761355  ]\n",
      " [17.946648    0.02306306 13.761355  ]\n",
      " [17.946648    0.02306306 13.761355  ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(np.matrix([[1, 2.0, 3]]), dtype=tf.float32)\n",
    "w = tf.Variable(tf.random.normal([3, 3], 2, 4))\n",
    "b = tf.zeros([3, 3])\n",
    "#------------------------\n",
    "print(\"\\nBefore adding b:\")\n",
    "print(tf.matmul(x, w))\n",
    "print(\"\\nAfter adding b: \")\n",
    "print(tf.matmul(x, w) + b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
